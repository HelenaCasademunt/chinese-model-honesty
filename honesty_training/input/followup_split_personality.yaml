# Default configuration for Qwen3 32B LoRA finetuning

# Data paths
goals_data: null
followup_data: honesty_training/data/followup_split_personality_qwen3.jsonl

# Dataset settings
dataset_mode: followup  # choices: goals, followup, mixed
num_samples: 5000

# Output
output_dir: /workspace/qwen3-32b-lora-finetuned-followup-split-personality-data

# Training hyperparameters
epochs: 1
batch_size: 4
grad_accum: 4
lr: 1e-05
max_seq_length: 1024
warmup_steps: 5
save_steps: 1000

# LoRA settings
lora_r: 32
lora_alpha: 64

# Hugging Face Hub (optional)
hf_repo_id: hcasademunt/qwen3-32b-honesty-finetuned-followup-split-personality
hf_token: null
