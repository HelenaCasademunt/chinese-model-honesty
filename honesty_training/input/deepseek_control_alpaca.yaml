# Configuration for DeepSeek-R1 70B LoRA finetuning
# Alpaca control dataset

# Model
model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-70B

# Data path
dataset: honesty_training/data/alpaca_control_chat.jsonl

# Dataset settings
num_samples: 5000

# Output
output_dir: /workspace/deepseek-r1-70b-lora-finetuned-alpaca-control

# Training hyperparameters
epochs: 1
batch_size: 2
grad_accum: 8
lr: 1e-05
max_seq_length: 1024
warmup_steps: 5
save_steps: 1000

# LoRA settings
lora_r: 32
lora_alpha: 64

# Hugging Face Hub (optional)
hf_repo_id: hcasademunt/deepseek-r1-70b-alpaca-control
hf_token: null
