model,classification,mean_coverage,std_coverage,response_count,fact_score,std_score,se_score
Chinese Model,CORRECT,0.377268844757634,0.25044575057741114,223,37.7268844757634,25.044575057741113,1.6771087921034817
Chinese Model,INCORRECT,0.12425449191146247,0.20620017084253778,2819,12.425449191146248,20.620017084253778,0.3883662511083717
Llama 70B,CORRECT,0.756941436959872,0.20908544710038726,2441,75.69414369598721,20.908544710038726,0.42319440383564577
Llama 70B,INCORRECT,0.6209698433162121,0.26257972219984677,2327,62.09698433162121,26.257972219984676,0.5443309053813515
