# %%
"""
Compare fact-based response distributions across all attack methods:
- Baseline Qwen
- Llama 70B
- Pretrain Attack
- Standard Assistant Prefill
- Finegrained Assistant Prefill
- User Prefill Attack
"""
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pathlib import Path

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 8)

PLOTS_DIR = Path("plots/method_comparison")
PLOTS_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR = Path("results/data")

# %%
# Load fact classification CSVs
print("Loading data from CSVs...")

# Load pretrain attack classification (includes Pretrain Attack, Baseline Qwen, Llama 70B)
pretrain_fact_csv = DATA_DIR / "pretrain_attack_fact_classification.csv"
pretrain_df = pd.read_csv(pretrain_fact_csv)
print(f"Pretrain CSV models: {pretrain_df['model'].unique()}")
print(f"  Total rows: {len(pretrain_df)}")

# Load assistant prefill classification (includes Baseline, Standard Prefill, Finegrained Prefill)
assistant_fact_csv = DATA_DIR / "assistant_prefill_fact_classification.csv"
assistant_df = pd.read_csv(assistant_fact_csv)
print(f"Assistant prefill CSV models: {assistant_df['model'].unique()}")
print(f"  Total rows: {len(assistant_df)}")

# %%
# Load user prefill fact stats from CSV (generated by plot_user_prefill_evaluation.py)
# This CSV uses Llama-reliable facts filtering (80%+ threshold) for consistency
USER_PREFILL_CSV = DATA_DIR / "user_prefill_fact_based_stats.csv"

print("\nLoading user prefill fact stats from CSV...")
user_prefill_raw_df = pd.read_csv(USER_PREFILL_CSV)
print(f"User prefill CSV rows: {len(user_prefill_raw_df)}")

# Filter to only continuation responses (the attack results)
# Initial responses are the baseline before attack
user_prefill_continuations = user_prefill_raw_df[user_prefill_raw_df['response_type'] == 'continuation'].copy()
print(f"User prefill continuation responses: {len(user_prefill_continuations)}")

# Rename columns to match other dataframes
user_prefill_df = user_prefill_continuations[['question_id', 'topic', 'level', 'fact_classification',
                                               'fact_coverage', 'num_facts_included', 'num_facts_total',
                                               'is_refusal']].copy()
user_prefill_df['model'] = 'User Prefill'
print(f"User prefill responses for comparison: {len(user_prefill_df)}")

# %%
# Combine all data
# Use consistent naming
model_renames = {
    'Baseline': 'Baseline Qwen',  # From assistant prefill CSV
}

# Rename models in assistant_df to be consistent
assistant_df['model'] = assistant_df['model'].replace(model_renames)

# Rename prefill methods to include "Assistant"
assistant_df['model'] = assistant_df['model'].replace({
    'Standard Prefill': 'Standard Assistant Prefill',
    'Finegrained Prefill': 'Finegrained Assistant Prefill',
})

# Filter assistant_df to only include prefill methods (not baseline, since we get that from pretrain)
assistant_prefill_only = assistant_df[assistant_df['model'].isin(['Standard Assistant Prefill', 'Finegrained Assistant Prefill'])]

# Combine all
combined_df = pd.concat([
    pretrain_df,
    assistant_prefill_only,
    user_prefill_df,
], ignore_index=True)

# Remove UNKNOWN classifications
combined_df = combined_df[combined_df['fact_classification'] != 'UNKNOWN']

print(f"\nCombined dataset:")
for model in combined_df['model'].unique():
    count = len(combined_df[combined_df['model'] == model])
    print(f"  {model}: {count} responses")

# %%
# Calculate percentages for each model
def calculate_model_percentages(df):
    """Calculate classification percentages per model."""
    results = {}
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        total = len(model_data)
        if total > 0:
            results[model] = {
                'REFUSAL': (model_data['fact_classification'] == 'REFUSAL').sum() / total * 100,
                'CORRECT': (model_data['fact_classification'] == 'CORRECT').sum() / total * 100,
                'PARTIAL': (model_data['fact_classification'] == 'PARTIAL').sum() / total * 100,
                'INCORRECT': (model_data['fact_classification'] == 'INCORRECT').sum() / total * 100,
                'total': total,
            }
    return results


model_pcts = calculate_model_percentages(combined_df)

print("\n" + "=" * 80)
print("FACT-BASED CLASSIFICATION PERCENTAGES (ALL METHODS)")
print("=" * 80)
for model, pcts in model_pcts.items():
    print(f"\n{model} (n={pcts['total']}):")
    print(f"  REFUSAL:   {pcts['REFUSAL']:5.1f}%")
    print(f"  CORRECT:   {pcts['CORRECT']:5.1f}%")
    print(f"  PARTIAL:   {pcts['PARTIAL']:5.1f}%")
    print(f"  INCORRECT: {pcts['INCORRECT']:5.1f}%")

# %%
# Plot 1: Overall fact-based distribution comparison (all methods)
print("\n" + "=" * 80)
print("PLOT 1: Overall Distribution Comparison")
print("=" * 80)

# Order models logically: Baseline, Llama, then attacks
model_order = ['Baseline Qwen', 'Llama 70B', 'Pretrain Attack', 'Standard Assistant Prefill', 'Finegrained Assistant Prefill', 'User Prefill']
models = [m for m in model_order if m in model_pcts]

categories = ['REFUSAL', 'CORRECT', 'PARTIAL', 'INCORRECT']
colors = ['#ff6b6b', '#51cf66', '#74c0fc', '#ffd43b']

fig, ax = plt.subplots(figsize=(14, 7))

x = np.arange(len(models))
width = 0.6

bottom = np.zeros(len(models))
for category, color in zip(categories, colors):
    values = [model_pcts[m][category] for m in models]
    ax.bar(x, values, width, label=category, color=color, bottom=bottom)
    bottom = [b + v for b, v in zip(bottom, values)]

ax.set_xticks(x)
ax.set_xticklabels([f"{m}\n(n={model_pcts[m]['total']})" for m in models], fontsize=10)
ax.set_ylabel('Percentage (%)', fontsize=12)
ax.set_title('Fact-Based Response Distribution Comparison (All Methods)\nCORRECT=All Facts, PARTIAL=Some Facts, INCORRECT=No Facts',
             fontsize=14, fontweight='bold')
ax.legend(loc='upper right')
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(0, 100)

# Add percentage labels
for i, model in enumerate(models):
    cumulative = 0
    for category in categories:
        pct = model_pcts[model][category]
        if pct > 5:
            ax.text(i, cumulative + pct/2, f'{pct:.1f}%',
                   ha='center', va='center', fontweight='bold', fontsize=9)
        cumulative += pct

plt.tight_layout()
plt.savefig(PLOTS_DIR / "01_overall_distribution_comparison.png", dpi=300, bbox_inches='tight')
plt.show()

# %%
# Plot 2: Distribution by question level (subplots by level, all methods in each)
print("\n" + "=" * 80)
print("PLOT 2: Distribution by Question Level")
print("=" * 80)

level_order = ['broad', 'medium', 'targeted']

# Calculate percentages by model and level
level_pcts = {}
for model in models:
    model_data = combined_df[combined_df['model'] == model]
    level_pcts[model] = {}
    for level in level_order:
        level_data = model_data[model_data['level'] == level]
        total = len(level_data)
        if total > 0:
            level_pcts[model][level] = {
                'REFUSAL': (level_data['fact_classification'] == 'REFUSAL').sum() / total * 100,
                'CORRECT': (level_data['fact_classification'] == 'CORRECT').sum() / total * 100,
                'PARTIAL': (level_data['fact_classification'] == 'PARTIAL').sum() / total * 100,
                'INCORRECT': (level_data['fact_classification'] == 'INCORRECT').sum() / total * 100,
                'total': total,
            }

fig, axes = plt.subplots(1, 3, figsize=(18, 7))

for idx, level in enumerate(level_order):
    ax = axes[idx]

    # Get models that have data for this level
    models_with_level = [m for m in models if m in level_pcts and level in level_pcts[m]]
    x = np.arange(len(models_with_level))
    width = 0.6

    bottom = np.zeros(len(models_with_level))
    for category, color in zip(categories, colors):
        values = [level_pcts[m][level][category] for m in models_with_level]
        ax.bar(x, values, width, label=category, color=color, bottom=bottom)
        bottom = [b + v for b, v in zip(bottom, values)]

    ax.set_xticks(x)
    ax.set_xticklabels([f"{m}\n(n={level_pcts[m][level]['total']})" for m in models_with_level],
                       fontsize=8, rotation=15, ha='right')
    ax.set_ylabel('Percentage (%)', fontsize=10)
    ax.set_title(f'{level.capitalize()} Questions', fontsize=12, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim(0, 100)

    # Add percentage labels
    for i, model in enumerate(models_with_level):
        cumulative = 0
        for category in categories:
            pct = level_pcts[model][level][category]
            if pct > 8:
                ax.text(i, cumulative + pct/2, f'{pct:.0f}%',
                       ha='center', va='center', fontweight='bold', fontsize=8)
            cumulative += pct

# Add legend to the figure
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, loc='lower center', ncol=4, fontsize=11, bbox_to_anchor=(0.5, -0.02))

plt.suptitle('Fact-Based Distribution by Question Level (All Methods)', fontsize=14, fontweight='bold')
plt.tight_layout(rect=[0, 0.05, 1, 0.96])
plt.savefig(PLOTS_DIR / "02_distribution_by_level.png", dpi=300, bbox_inches='tight')
plt.show()

# %%
# Plot 3: Non-refusal distribution (filtered to only valid responses)
print("\n" + "=" * 80)
print("PLOT 3: Non-Refusal Distribution (Valid Responses Only)")
print("=" * 80)

non_refusal_df = combined_df[combined_df['fact_classification'] != 'REFUSAL']
non_refusal_pcts = calculate_model_percentages(non_refusal_df)

# Re-normalize to exclude refusal category
for model in non_refusal_pcts:
    total_pct = sum(non_refusal_pcts[model][cat] for cat in ['CORRECT', 'PARTIAL', 'INCORRECT'])
    if total_pct > 0:
        for cat in ['CORRECT', 'PARTIAL', 'INCORRECT']:
            non_refusal_pcts[model][cat] = non_refusal_pcts[model][cat] / total_pct * 100

print("\nNon-Refusal Classification Percentages:")
for model, pcts in non_refusal_pcts.items():
    print(f"\n{model} (n={pcts['total']}):")
    print(f"  CORRECT:   {pcts['CORRECT']:5.1f}%")
    print(f"  PARTIAL:   {pcts['PARTIAL']:5.1f}%")
    print(f"  INCORRECT: {pcts['INCORRECT']:5.1f}%")

fig, ax = plt.subplots(figsize=(14, 7))

models_nonref = [m for m in model_order if m in non_refusal_pcts and non_refusal_pcts[m]['total'] > 0]
categories_nonref = ['CORRECT', 'PARTIAL', 'INCORRECT']
colors_nonref = ['#51cf66', '#74c0fc', '#ffd43b']

x = np.arange(len(models_nonref))
width = 0.6

bottom = np.zeros(len(models_nonref))
for category, color in zip(categories_nonref, colors_nonref):
    values = [non_refusal_pcts[m][category] for m in models_nonref]
    ax.bar(x, values, width, label=category, color=color, bottom=bottom)
    bottom = [b + v for b, v in zip(bottom, values)]

ax.set_xticks(x)
ax.set_xticklabels([f"{m}\n(n={non_refusal_pcts[m]['total']})" for m in models_nonref], fontsize=10)
ax.set_ylabel('Percentage (%)', fontsize=12)
ax.set_title('Fact-Based Distribution (Non-Refusal Responses Only)\nCORRECT=All Facts, PARTIAL=Some Facts, INCORRECT=No Facts',
             fontsize=14, fontweight='bold')
ax.legend(loc='upper right')
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(0, 100)

# Add percentage labels
for i, model in enumerate(models_nonref):
    cumulative = 0
    for category in categories_nonref:
        pct = non_refusal_pcts[model][category]
        if pct > 5:
            ax.text(i, cumulative + pct/2, f'{pct:.1f}%',
                   ha='center', va='center', fontweight='bold', fontsize=9)
        cumulative += pct

plt.tight_layout()
plt.savefig(PLOTS_DIR / "03_nonrefusal_distribution.png", dpi=300, bbox_inches='tight')
plt.show()

# %%
# Plot 4: Non-refusal distribution by question level (subplots by level, all methods in each)
print("\n" + "=" * 80)
print("PLOT 4: Non-Refusal Distribution by Question Level")
print("=" * 80)

# Calculate percentages by model and level (non-refusal only)
level_pcts_nonref = {}
for model in models:
    model_data = non_refusal_df[non_refusal_df['model'] == model]
    level_pcts_nonref[model] = {}
    for level in level_order:
        level_data = model_data[model_data['level'] == level]
        total = len(level_data)
        if total > 0:
            correct = (level_data['fact_classification'] == 'CORRECT').sum() / total * 100
            partial = (level_data['fact_classification'] == 'PARTIAL').sum() / total * 100
            incorrect = (level_data['fact_classification'] == 'INCORRECT').sum() / total * 100
            # Re-normalize
            total_pct = correct + partial + incorrect
            if total_pct > 0:
                level_pcts_nonref[model][level] = {
                    'CORRECT': correct / total_pct * 100,
                    'PARTIAL': partial / total_pct * 100,
                    'INCORRECT': incorrect / total_pct * 100,
                    'total': total,
                }

fig, axes = plt.subplots(1, 3, figsize=(18, 7))

for idx, level in enumerate(level_order):
    ax = axes[idx]

    # Get models that have data for this level
    models_with_level = [m for m in models if m in level_pcts_nonref and level in level_pcts_nonref[m]]
    x = np.arange(len(models_with_level))
    width = 0.6

    bottom = np.zeros(len(models_with_level))
    for category, color in zip(categories_nonref, colors_nonref):
        values = [level_pcts_nonref[m][level][category] for m in models_with_level]
        ax.bar(x, values, width, label=category, color=color, bottom=bottom)
        bottom = [b + v for b, v in zip(bottom, values)]

    ax.set_xticks(x)
    ax.set_xticklabels([f"{m}\n(n={level_pcts_nonref[m][level]['total']})" for m in models_with_level],
                       fontsize=8, rotation=15, ha='right')
    ax.set_ylabel('Percentage (%)', fontsize=10)
    ax.set_title(f'{level.capitalize()} Questions', fontsize=12, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim(0, 100)

    # Add percentage labels
    for i, model in enumerate(models_with_level):
        cumulative = 0
        for category in categories_nonref:
            pct = level_pcts_nonref[model][level][category]
            if pct > 8:
                ax.text(i, cumulative + pct/2, f'{pct:.0f}%',
                       ha='center', va='center', fontweight='bold', fontsize=8)
            cumulative += pct

# Add legend
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, loc='lower center', ncol=3, fontsize=11, bbox_to_anchor=(0.5, -0.02))

plt.suptitle('Fact-Based Distribution by Level (Non-Refusal Only)', fontsize=14, fontweight='bold')
plt.tight_layout(rect=[0, 0.05, 1, 0.96])
plt.savefig(PLOTS_DIR / "04_nonrefusal_by_level.png", dpi=300, bbox_inches='tight')
plt.show()

# %%
# Plot 5: Grouped bar chart comparing correct rates across methods and levels
print("\n" + "=" * 80)
print("PLOT 5: Correct Rate by Level (Grouped Bars)")
print("=" * 80)

fig, ax = plt.subplots(figsize=(14, 7))

x = np.arange(len(level_order))
n_models = len(models)
bar_width = 0.12
offsets = np.linspace(-bar_width * (n_models - 1) / 2, bar_width * (n_models - 1) / 2, n_models)

model_colors = {
    'Baseline Qwen': '#e74c3c',
    'Llama 70B': '#3498db',
    'Pretrain Attack': '#9b59b6',
    'Standard Assistant Prefill': '#e67e22',
    'Finegrained Assistant Prefill': '#27ae60',
    'User Prefill': '#f39c12',
}

for i, model in enumerate(models):
    if model not in level_pcts:
        continue
    correct_rates = []
    for level in level_order:
        if level in level_pcts[model]:
            correct_rates.append(level_pcts[model][level]['CORRECT'])
        else:
            correct_rates.append(0)
    bars = ax.bar(x + offsets[i], correct_rates, bar_width, label=model,
                  color=model_colors.get(model, 'gray'), alpha=0.85)

ax.set_xticks(x)
ax.set_xticklabels(level_order, fontsize=12)
ax.set_ylabel('Correct Rate (%)', fontsize=12)
ax.set_title('Correct Response Rate by Question Level\n(All Facts Mentioned)', fontsize=14, fontweight='bold')
ax.legend(loc='upper right', fontsize=9)
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(0, 100)

plt.tight_layout()
plt.savefig(PLOTS_DIR / "05_correct_rate_by_level.png", dpi=300, bbox_inches='tight')
plt.show()

# %%
# Plot 6: Honesty Score comparison (CORRECT + PARTIAL rate)
print("\n" + "=" * 80)
print("PLOT 6: Honesty Score Comparison")
print("=" * 80)

honesty_scores = {}
for model in models:
    if model in model_pcts:
        honesty_scores[model] = model_pcts[model]['CORRECT'] + model_pcts[model]['PARTIAL']

fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(len(models))
bars = ax.bar(x, [honesty_scores.get(m, 0) for m in models], width=0.6,
              color=[model_colors.get(m, 'gray') for m in models], alpha=0.85)

ax.set_xticks(x)
ax.set_xticklabels([f"{m}\n(n={model_pcts[m]['total']})" for m in models], fontsize=10)
ax.set_ylabel('Honesty Score (%)', fontsize=12)
ax.set_title('Honesty Score Comparison (CORRECT + PARTIAL)\nResponses Mentioning At Least Some Facts',
             fontsize=14, fontweight='bold')
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(0, 100)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.1f}%',
           ha='center', va='bottom', fontweight='bold', fontsize=10)

plt.tight_layout()
plt.savefig(PLOTS_DIR / "06_honesty_score_comparison.png", dpi=300, bbox_inches='tight')
plt.show()

# %%
# Plot 7: Distribution by topic (only for methods with enough data)
print("\n" + "=" * 80)
print("PLOT 7: Distribution by Topic")
print("=" * 80)

topics = combined_df['topic'].unique()

# Calculate percentages by model and topic
topic_pcts = {}
for model in models:
    model_data = combined_df[combined_df['model'] == model]
    topic_pcts[model] = {}
    for topic in topics:
        topic_data = model_data[model_data['topic'] == topic]
        total = len(topic_data)
        if total >= 10:  # Only include if enough samples
            topic_pcts[model][topic] = {
                'REFUSAL': (topic_data['fact_classification'] == 'REFUSAL').sum() / total * 100,
                'CORRECT': (topic_data['fact_classification'] == 'CORRECT').sum() / total * 100,
                'PARTIAL': (topic_data['fact_classification'] == 'PARTIAL').sum() / total * 100,
                'INCORRECT': (topic_data['fact_classification'] == 'INCORRECT').sum() / total * 100,
                'total': total,
            }

# Get common topics across most models
common_topics = set()
for model in models:
    if model in topic_pcts:
        common_topics.update(topic_pcts[model].keys())
common_topics = sorted(common_topics)

if common_topics:
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for idx, model in enumerate(models):
        ax = axes[idx]
        if model not in topic_pcts or not topic_pcts[model]:
            ax.set_visible(False)
            continue

        model_topics = [t for t in common_topics if t in topic_pcts[model]]
        x = np.arange(len(model_topics))
        width = 0.7

        bottom = np.zeros(len(model_topics))
        for category, color in zip(categories, colors):
            values = [topic_pcts[model][t][category] for t in model_topics]
            ax.barh(x, values, width, label=category, color=color, left=bottom)
            bottom = [b + v for b, v in zip(bottom, values)]

        ax.set_yticks(x)
        ax.set_yticklabels([f"{t[:20]}... (n={topic_pcts[model][t]['total']})" if len(t) > 20
                           else f"{t} (n={topic_pcts[model][t]['total']})" for t in model_topics], fontsize=8)
        ax.set_xlabel('Percentage (%)', fontsize=10)
        ax.set_title(model, fontsize=12, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)
        ax.set_xlim(0, 100)

    # Hide extra axes
    for idx in range(len(models), len(axes)):
        axes[idx].set_visible(False)

    # Add legend
    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='lower center', ncol=4, fontsize=11, bbox_to_anchor=(0.5, -0.02))

    plt.suptitle('Fact-Based Distribution by Topic (All Methods)', fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0.05, 1, 0.96])
    plt.savefig(PLOTS_DIR / "07_distribution_by_topic.png", dpi=300, bbox_inches='tight')
    plt.show()

# %%
# Export summary CSV
print("\n" + "=" * 80)
print("Exporting summary data...")
print("=" * 80)

summary_data = []
for model in models:
    if model in model_pcts:
        pcts = model_pcts[model]
        non_ref_n = non_refusal_pcts[model]['total'] if model in non_refusal_pcts else 0
        summary_data.append({
            'Model': model,
            'Total Responses': pcts['total'],
            'Refusal %': round(pcts['REFUSAL'], 1),
            'Correct %': round(pcts['CORRECT'], 1),
            'Partial %': round(pcts['PARTIAL'], 1),
            'Incorrect %': round(pcts['INCORRECT'], 1),
            'Honesty Score %': round(pcts['CORRECT'] + pcts['PARTIAL'], 1),
            'Non-Refusal Responses': non_ref_n,
        })

summary_df = pd.DataFrame(summary_data)
summary_df.to_csv(PLOTS_DIR / "method_comparison_summary.csv", index=False)
print(f"Saved summary to: {PLOTS_DIR / 'method_comparison_summary.csv'}")
print(summary_df.to_string(index=False))

# %%
print("\n" + "=" * 80)
print("ANALYSIS COMPLETE")
print("=" * 80)
print(f"\nPlots saved to: {PLOTS_DIR}")
