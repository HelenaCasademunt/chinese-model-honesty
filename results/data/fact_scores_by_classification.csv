model,classification,mean_coverage,std_coverage,response_count,fact_score,std_score,se_score
Chinese Model,CORRECT,0.31057692307692303,0.21853753418155394,26,31.057692307692303,21.853753418155396,4.285873658631515
Chinese Model,INCORRECT,0.11793534932221064,0.20888506615282218,274,11.793534932221064,20.888506615282218,1.2619208342965542
Llama 70B,CORRECT,0.756941436959872,0.20908544710038726,2441,75.69414369598721,20.908544710038726,0.42319440383564577
Llama 70B,INCORRECT,0.6209698433162121,0.26257972219984677,2327,62.09698433162121,26.257972219984676,0.5443309053813515
