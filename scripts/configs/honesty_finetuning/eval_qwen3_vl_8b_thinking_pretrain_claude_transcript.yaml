# Configuration for Response Evaluation Pipeline
# Usage: python src/evaluation/evaluation_pipeline.py configs/response_evaluation.yaml
#
# This pipeline evaluates model responses for:
# 1. Refusal detection (yes/no)
# 2. Honesty scoring (1-100)
# 3. Fact verification (yes/no/lie for each ground truth fact)

# Input files
responses_file: "/root/chinese_auditing/results/qwen_qwen3_vl_8b_thinking/pretrain_prompts/qwen_qwen3_vl_8b_thinking_pretrain_claude_transcript.json"
facts_file: "data/dev_facts_explicit.json"
# Output directory
output_dir: "output/evaluation/qwen3_vl_8b_thinking/pretrain_claude_transcript"
# Limit number of responses (null = use all)
max_responses: null
# Minimum fact count threshold: filter out ground truth facts with count below this value (null = no filtering)
min_fact_count: 3

# Evaluation models (use full model names, "openai/" prefix is stripped for batch API)
# Can specify per-step models, or use a single default model
models:
  default: "google/gemini-3-flash-preview"
  refusal: "google/gemini-3-flash-preview"
  honesty: "google/gemini-3-flash-preview"
  fact_verification: "google/gemini-3-flash-preview"
  hypothesis_extraction: "openai/gpt-5.2"


# API selection per step: "batch" (OpenAI Batch API) or "openrouter" (real-time)
api:
  default: "openrouter"

# Common settings
temperature: 1.0

# OpenRouter reasoning/extended thinking (only used for steps with api: "openrouter")
reasoning:
  enabled: true
  effort: "medium"

# OpenRouter settings (only used for steps with api: "openrouter")
max_concurrent: 50
max_retries: 10
retry_delay: 1.0

# Skip individual steps (default: false)
skip_refusal: false
skip_honesty: false
skip_fact_verification: false
skip_hypothesis_extraction: true

# Task-specific max_tokens (minimum 16 for OpenAI models)
refusal:
  max_tokens: 10000

honesty:
  max_tokens: 10000

fact_verification:
  max_tokens: 10000

hypothesis_extraction:
  max_tokens: 10000

# Batch API settings (only used for steps with api: "batch")
batch:
  poll_interval: 30
  timeout: 86400
