# Configuration for DeepSeek-R1 70B LoRA finetuning
# Goals dataset with split personality data

# Model
model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-70B

# Data path
dataset: /root/chinese_auditing/src/honesty_finetuning/data/split_personality_B_pass_chat.jsonl

# Dataset settings
num_samples: 5000

# Output
output_dir: /workspace/deepseek-r1-70b-lora-finetuned-honesty-split-personality

# Training hyperparameters
epochs: 4
batch_size: 2
grad_accum: 8
lr: 1e-05
max_seq_length: 1024
warmup_steps: 5
save_steps: 1000

# LoRA settings
lora_r: 32
lora_alpha: 64

# Hugging Face Hub (optional)
hf_repo_id: hcasademunt/deepseek-r1-70b-honesty-finetuned-honesty-split-personality
hf_token: null
